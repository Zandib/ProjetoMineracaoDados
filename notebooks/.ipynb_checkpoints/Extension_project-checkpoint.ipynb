{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRIPT DE TREINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training files...\n",
      "Extract training features...\n",
      "class a extracted\n",
      "class b extracted\n",
      "class c extracted\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: dib_n\n",
    "\"\"\"\n",
    "#################################################################\n",
    "#Imports\n",
    "#################################################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "#Learning\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#SearchGrid\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#Saving \n",
    "from sklearn.externals import joblib \n",
    "\n",
    "#################################################################\n",
    "#Prep train/validation set from external file\n",
    "#################################################################\n",
    "exec(open('../scripts/dataprep_treino_validacao.py').read())\n",
    "#################################################################\n",
    "#Loads\n",
    "#################################################################\n",
    "#Importando base de treino\n",
    "df_train = pd.read_csv('../data/data.csv',index_col=False)\n",
    "df_valid = pd.read_csv('../data/data_validacao.csv',index_col=False)\n",
    "# Dropping unneccesary columns\n",
    "df_train = df_train.drop(['filename'],axis=1)\n",
    "df_valid = df_valid.drop('filename',axis=1)\n",
    "#################################################################\n",
    "#Encoding\n",
    "#################################################################\n",
    "class_list = df_train.iloc[:, -1]\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(class_list)\n",
    "y_train = encoder.transform(class_list)\n",
    "y_valid = encoder.transform(df_valid.iloc[:,-1])\n",
    "joblib.dump(encoder,'../models/encoder.pkl')\n",
    "#################################################################\n",
    "#Scailing\n",
    "#################################################################\n",
    "scaler = StandardScaler()\n",
    "df = df_train.append(df_valid,ignore_index=True)\n",
    "scaler.fit(np.array(df.iloc[:, :-1], dtype = float))\n",
    "X_train = scaler.transform(np.array(df_train.iloc[:, :-1], dtype = float))\n",
    "X_valid = scaler.transform(np.array(df_valid.iloc[:, :-1], dtype = float))\n",
    "joblib.dump(scaler,'../models/scaler.pkl')\n",
    "#################################################################\n",
    "#Training svm with two different kernels\n",
    "#################################################################\n",
    "#Linear\n",
    "SVClassifier = svm.SVC(kernel='linear')\n",
    "SVClassifier.fit(X_train,y_train)\n",
    "print('Accuracy of linear SVM:',SVClassifier.score(X_valid,y_valid))\n",
    "joblib.dump(SVClassifier,'../models/svclinear.pkl')\n",
    "#Rbf\n",
    "##Param grid\n",
    "Cs = np.arange(0.5,100,0.5)\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "\n",
    "param_grid = {'C':Cs,'gamma':gammas}\n",
    "#Grid Search\n",
    "print('Tuning RBF Kernel parameters')\n",
    "grid_search = GridSearchCV(svm.SVC(kernel='rbf'),param_grid)\n",
    "grid_search.fit(X_train,y_train)\n",
    "print('Search grid for RBF returned parameters:')\n",
    "print(grid_search.best_params_)\n",
    "#Get model  \n",
    "SVCrbf = grid_search.best_estimator_\n",
    "print('Accuracy score of RBF Kernel:',SVCrbf.score(X_valid,y_valid))\n",
    "joblib.dump(SVCrbf, '../models/svcrbf.pkl') \n",
    "##################################################################\n",
    "#Re-treino com validação\n",
    "##################################################################\n",
    "print('Re-training with validation data')\n",
    "X = np.concatenate((X_train,X_valid))\n",
    "y = np.concatenate((y_train,y_valid))\n",
    "\n",
    "SVCrbf = joblib.load('../models/svcrbf.pkl')\n",
    "SVCrbf.fit(X,y)\n",
    "#Scoring\n",
    "scores = cross_val_score(SVCrbf, X, y, cv = 3)\n",
    "print('Training + Validation score(cross-val)',scores.mean())\n",
    "##################################\n",
    "#Paradigma One vs All\n",
    "##################################\n",
    "print('Implementing OneVsAll')\n",
    "#Separando uma coluna para cada target\n",
    "for target in df['label'].unique():\n",
    "    df[target] = (df['label']==target).astype(int)\n",
    "    df_train[target] = (df_train['label']==target).astype(int)\n",
    "    df_valid[target] = (df_valid['label']==target).astype(int)\n",
    "#Treinando um modelo para cada classe\n",
    "models={}\n",
    "    \n",
    "models['geral'] = joblib.load('../models/SVCrbf.pkl')\n",
    "for target in df['label'].unique():\n",
    "    SVCrbf = svm.SVC(\n",
    "        kernel='rbf',\n",
    "        C=3.0,\n",
    "        gamma=1.0,\n",
    "        probability=False\n",
    "    )\n",
    "    SVCrbf.fit(X_train,df_train[target])\n",
    "    print(\"Acuracia para a classe \"+target+\":\",SVCrbf.score(X_valid,df_valid[target]))\n",
    "    X = np.concatenate((X_train,X_valid))\n",
    "    y = np.concatenate((df_train[target],df_valid[target]))\n",
    "    SVCrbf.fit(X,y)\n",
    "    #Scoring\n",
    "    scores = cross_val_score(SVCrbf, X, y, cv = 3)\n",
    "    print('Training + Validation score(cross-val) para classe '+target+':',scores.mean())\n",
    "    \n",
    "    SVCrbf = joblib.load('../models/svcrbf.pkl')\n",
    "    SVCrbf.fit(X,y)\n",
    "    joblib.dump(SVCrbf,'../models/SVCrbf_'+target+'.pkl')\n",
    "    models[target]=joblib.load('../models/SVCrbf_'+target+'.pkl')\n",
    "#Ok, agora vamos criar uma nova base com a predição de todos os modelos\n",
    "stacking_df_train = pd.DataFrame(columns=list(models.keys()))\n",
    "stacking_df_valid = pd.DataFrame(columns=list(models.keys()))\n",
    "for item in models.items():\n",
    "    label=item[0]\n",
    "    model = item[1]\n",
    "    values_train = model.predict(X_train)\n",
    "    stacking_df_train[label]=values_train\n",
    "    values_valid = model.predict(X_valid)\n",
    "    stacking_df_valid[label]=values_valid\n",
    "stacking_df_train['label']=y_train\n",
    "stacking_df_valid['label']=y_valid\n",
    "X_train = stacking_df_train.iloc[:,:-1]\n",
    "X_valid = stacking_df_valid.iloc[:,:-1]\n",
    "y_train = stacking_df_train.iloc[:,-1]\n",
    "y_valid = stacking_df_valid.iloc[:,-1]\n",
    "X = np.concatenate((X_train,X_valid))\n",
    "y = np.concatenate((y_train,y_valid))\n",
    "#Param Grid\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]# Create the random grid\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'bootstrap': bootstrap}\n",
    "#RandomSearch\n",
    "# Use the random grid to search for best hyperparameters\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator = rf,\n",
    "    param_distributions = random_grid,\n",
    "    n_iter = 100,\n",
    "    random_state=42,\n",
    "    n_jobs = -1)# Fit the random search model\n",
    "\n",
    "rf_random.fit(X_train, y_train)\n",
    "RFC = rf_random.best_estimator_\n",
    "print('Acuracia para a base de validação:',RFC.Score(X_valid,y_valid))\n",
    "print('Retreinando com validação...')\n",
    "RFC.fit(X,y)\n",
    "#Scoring\n",
    "scores = cross_val_score(RFC, X, y, cv = 10)\n",
    "print('Training + Validation score(cross-val):',scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/scaler.pkl']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################\n",
    "#Paradigma One vs All\n",
    "##################################\n",
    "#DataFrame com as features extraidas\n",
    "\n",
    "#Importando base de treino\n",
    "df_train = pd.read_csv('../data/data.csv',index_col=False)\n",
    "df_valid = pd.read_csv('../data/data_validacao.csv',index_col=False)\n",
    "# Dropping unneccesary columns\n",
    "df_train = df_train.drop(['filename'],axis=1)\n",
    "df_valid = df_valid.drop('filename',axis=1)\n",
    "#################################################################\n",
    "#Encoding\n",
    "#################################################################\n",
    "class_list = df_train.iloc[:, -1]\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(class_list)\n",
    "joblib.dump(encoder,'../models/encoder.pkl')\n",
    "#################################################################\n",
    "#Scailing\n",
    "#################################################################\n",
    "scaler = StandardScaler()\n",
    "df = df_train.append(df_valid,ignore_index=True)\n",
    "scaler.fit(np.array(df.iloc[:, :-1], dtype = float))\n",
    "X_train = scaler.transform(np.array(df_train.iloc[:, :-1], dtype = float))\n",
    "X_valid = scaler.transform(np.array(df_valid.iloc[:, :-1], dtype = float))\n",
    "joblib.dump(scaler,'../models/scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Separando uma coluna para cada target\n",
    "for target in df['label'].unique():\n",
    "    df[target] = (df['label']==target).astype(int)\n",
    "    df_train[target] = (df_train['label']==target).astype(int)\n",
    "    df_valid[target] = (df_valid['label']==target).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracia para a classe a: 0.9101123595505618\n",
      "Training + Validation score(cross-val) para classe a: 0.9101121694962764\n",
      "Acuracia para a classe b: 0.900749063670412\n",
      "Training + Validation score(cross-val) para classe b: 0.8944643960457214\n",
      "Acuracia para a classe c: 0.900749063670412\n",
      "Training + Validation score(cross-val) para classe c: 0.9028890532043939\n",
      "Acuracia para a classe d: 0.897003745318352\n",
      "Training + Validation score(cross-val) para classe d: 0.9008843064565956\n",
      "Acuracia para a classe h: 0.8867041198501873\n",
      "Training + Validation score(cross-val) para classe h: 0.89646953197405\n",
      "Acuracia para a classe m: 0.9026217228464419\n",
      "Training + Validation score(cross-val) para classe m: 0.8992774950584529\n",
      "Acuracia para a classe n: 0.8941947565543071\n",
      "Training + Validation score(cross-val) para classe n: 0.9004813477737664\n",
      "Acuracia para a classe x: 0.9110486891385767\n",
      "Training + Validation score(cross-val) para classe x: 0.9065010946312325\n",
      "Acuracia para a classe 6: 0.9063670411985019\n",
      "Training + Validation score(cross-val) para classe 6: 0.8996794871794872\n",
      "Acuracia para a classe 7: 0.8932584269662921\n",
      "Training + Validation score(cross-val) para classe 7: 0.8968697896278255\n"
     ]
    }
   ],
   "source": [
    "#Treinando um modelo para cada classe\n",
    "models={}\n",
    "    \n",
    "models['geral'] = joblib.load('../models/SVCrbf.pkl')\n",
    "for target in df['label'].unique():\n",
    "    SVCrbf = svm.SVC(\n",
    "        kernel='rbf',\n",
    "        C=3.0,\n",
    "        gamma=1.0,\n",
    "        probability=False\n",
    "    )\n",
    "    SVCrbf.fit(X_train,df_train[target])\n",
    "    print(\"Acuracia para a classe \"+target+\":\",SVCrbf.score(X_valid,df_valid[target]))\n",
    "    X = np.concatenate((X_train,X_valid))\n",
    "    y = np.concatenate((df_train[target],df_valid[target]))\n",
    "    SVCrbf.fit(X,y)\n",
    "    #Scoring\n",
    "    scores = cross_val_score(SVCrbf, X, y, cv = 3)\n",
    "    print('Training + Validation score(cross-val) para classe '+target+':',scores.mean())\n",
    "    \n",
    "    SVCrbf = joblib.load('../models/svcrbf.pkl')\n",
    "    SVCrbf.fit(X,y)\n",
    "    joblib.dump(SVCrbf,'../models/SVCrbf_'+target+'.pkl')\n",
    "    models[target]=joblib.load('../models/SVCrbf_'+target+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ok, agora vamos criar uma nova base com a predição de todos os modelos\n",
    "stacking_df_train = pd.DataFrame(columns=list(models.keys()))\n",
    "stacking_df_valid = pd.DataFrame(columns=list(models.keys()))\n",
    "for item in models.items():\n",
    "    label=item[0]\n",
    "    model = item[1]\n",
    "    values_train = model.predict(X_train)\n",
    "    stacking_df_train[label]=values_train\n",
    "    values_valid = model.predict(X_valid)\n",
    "    stacking_df_valid[label]=values_valid\n",
    "stacking_df_train['label']=y_train\n",
    "stacking_df_valid['label']=y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train = stacking_df_train.iloc[:,:-1]\n",
    "X_valid = stacking_df_valid.iloc[:,:-1]\n",
    "y_train = stacking_df_train.iloc[:,-1]\n",
    "y_valid = stacking_df_valid.iloc[:,-1]\n",
    "X = np.concatenate((X_train,X_valid))\n",
    "y = np.concatenate((y_train,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Param Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]# Create the random grid\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators='warn',\n",
       "                                                    n_jobs=None,\n",
       "                                                    o...\n",
       "                                                    verbose=0,\n",
       "                                                    warm_start=False),\n",
       "                   iid='warn', n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator = rf,\n",
    "    param_distributions = random_grid,\n",
    "    n_iter = 100,\n",
    "    random_state=42,\n",
    "    n_jobs = -1)# Fit the random search model\n",
    "\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC = rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': 40,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 5,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 400,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFC.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9054307116104869"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFC.score(X_valid,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importando base de treino\n",
    "df_train = pd.read_csv('../data/data.csv',index_col=False)\n",
    "df_valid = pd.read_csv('../data/data_validacao.csv',index_col=False)\n",
    "# Dropping unneccesary columns\n",
    "df_train = df_train.drop(['filename'],axis=1)\n",
    "df_valid = df_valid.drop('filename',axis=1)\n",
    "#################################################################\n",
    "#Encoding\n",
    "#################################################################\n",
    "class_list = df_train.iloc[:, -1]\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(class_list)\n",
    "y_train = encoder.transform(class_list)\n",
    "y_valid = encoder.transform(df_valid.iloc[:,-1])\n",
    "joblib.dump(encoder,'../models/encoder.pkl')\n",
    "#################################################################\n",
    "#Scailing\n",
    "#################################################################\n",
    "scaler = StandardScaler()\n",
    "df = df_train.append(df_valid,ignore_index=True)\n",
    "scaler.fit(np.array(df.iloc[:, :-1], dtype = float))\n",
    "X_train = scaler.transform(np.array(df_train.iloc[:, :-1], dtype = float))\n",
    "X_valid = scaler.transform(np.array(df_valid.iloc[:, :-1], dtype = float))\n",
    "joblib.dump(scaler,'../models/scaler.pkl')\n",
    "#################################################################\n",
    "#Training svm with two different kernels\n",
    "#################################################################\n",
    "#Linear\n",
    "#SVClassifier = svm.SVC(kernel='linear')\n",
    "#SVClassifier.fit(X_train,y_train)\n",
    "#print('Accuracy of linear SVM:',SVClassifier.score(X_valid,y_valid))\n",
    "#joblib.dump(SVClassifier,'../models/svclinear.pkl')\n",
    "#################################################################\n",
    "#Divisor\n",
    "divisor = StratifiedKFold(n_splits = 5, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ufabc/anaconda3/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator SVC from version 0.20.1 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((X_train,X_valid))\n",
    "y = np.concatenate((y_train,y_valid))\n",
    "\n",
    "SVCrbf = joblib.load('../models/svcrbf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=SVCrbf.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['6', '7', 'a', 'b', 'c', 'd', 'h', 'm', 'n', 'x'], dtype='<U1')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           6       0.92      0.96      0.94       122\n",
      "           7       0.51      0.52      0.52       104\n",
      "           a       0.00      0.00      0.00         0\n",
      "           b       0.00      0.00      0.00         0\n",
      "           c       0.00      0.00      0.00         0\n",
      "           d       0.00      0.00      0.00         0\n",
      "           h       0.00      0.00      0.00         0\n",
      "           m       0.00      0.00      0.00         0\n",
      "           n       0.00      0.00      0.00         0\n",
      "           x       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.74      0.76      0.75       226\n",
      "   macro avg       0.14      0.15      0.15       226\n",
      "weighted avg       0.73      0.76      0.75       226\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ufabc/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py:564: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n",
      "/home/ufabc/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ufabc/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid,predictions,labels=encoder.classes_.astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-training with validation data\n",
      "Score for validation + training set: 0.9530497592295345\n"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "#Re-treino com validação\n",
    "##################################################################\n",
    "print('Re-training with validation data')\n",
    "SVCrbf.fit(X,y)\n",
    "#Scoring\n",
    "scores = cross_val_score(SVCrbf, X, y, cv = 3)\n",
    "print('Training + Validation score(cross-val)',scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
